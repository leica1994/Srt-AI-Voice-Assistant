"""
ä¼˜åŒ–çš„éŸ³é¢‘åˆ†ç¦»æ¨¡å—
"""

import os
import gc
import re
import datetime
import warnings
import psutil
from typing import Tuple, Dict, List
from pathlib import Path
import tempfile
import shutil

# éšè— PyTorch å’Œ CUDA ç›¸å…³è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning, module="torch")
warnings.filterwarnings("ignore", message=".*cudnn.*")
warnings.filterwarnings("ignore", message=".*flash attention.*")
warnings.filterwarnings("ignore", message=".*Plan failed.*")

import ffmpeg
from demucs.pretrained import get_model
from demucs.apply import apply_model
import torch
import torchaudio
from pydub import AudioSegment

# è®¾ç½® PyTorch ä¼˜åŒ–é€‰é¡¹
if torch.cuda.is_available():
    torch.backends.cudnn.allow_tf32 = True
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.benchmark = True


class DemucsWrapper:
    """Demucs åˆ†ç¦»å™¨åŒ…è£…ç±» - ä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨ï¼Œé’ˆå¯¹16GBæ˜¾å¡"""

    def __init__(self, model_name: str = "htdemucs", max_memory_gb: float = 8.0):
        self.model_name = model_name
        self.model = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.max_memory_gb = max_memory_gb
        self.chunk_size = self._calculate_chunk_size()
        self.aggressive_cleanup = True  # å¯ç”¨ç§¯æçš„æ˜¾å­˜æ¸…ç†

        # æ£€æµ‹æ˜¯å¦ä¸º16GBæ˜¾å¡ä»¥å¯ç”¨é«˜è´¨é‡æ¨¡å¼
        self.high_quality_mode = False
        if torch.cuda.is_available():
            try:
                total_gpu_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                self.high_quality_mode = total_gpu_gb >= 15
            except:
                pass

        print(f"ğŸ¤– Initializing Demucs model: {model_name} on {self.device}")
        print(f"ğŸ’¾ Memory limit: {max_memory_gb}GB, Chunk size: {self.chunk_size}s")
        print(f"ğŸ§¹ Aggressive GPU cleanup: {'Enabled' if self.aggressive_cleanup else 'Disabled'}")
        print(f"ğŸ¯ High quality mode: {'Enabled' if self.high_quality_mode else 'Disabled'}")

    def _get_cpu_memory_info(self):
        """è·å–CPUå†…å­˜ä¿¡æ¯"""
        try:
            memory = psutil.virtual_memory()
            total_gb = memory.total / (1024**3)
            available_gb = memory.available / (1024**3)
            return total_gb, available_gb
        except Exception as e:
            print(f"âš ï¸ Could not detect CPU memory: {e}")
            return 16.0, 8.0  # ä¿å®ˆä¼°è®¡

    def _calculate_chunk_size(self):
        """æ ¹æ®å¯ç”¨æ˜¾å­˜å’Œå†…å­˜è®¡ç®—åˆé€‚çš„éŸ³é¢‘å—å¤§å° - é’ˆå¯¹16GBæ˜¾å¡ä¼˜åŒ–"""
        # è·å–CPUå†…å­˜ä¿¡æ¯
        total_cpu_gb, available_cpu_gb = self._get_cpu_memory_info()

        if self.device == "cpu":
            # CPUæ¨¡å¼ï¼šæ ¹æ®å¯ç”¨å†…å­˜è°ƒæ•´å—å¤§å°
            if available_cpu_gb >= 8:
                chunk_size = 45  # 8GB+: 45ç§’å—
            elif available_cpu_gb >= 4:
                chunk_size = 30  # 4-8GB: 30ç§’å—
            elif available_cpu_gb >= 2:
                chunk_size = 20  # 2-4GB: 20ç§’å—
            else:
                chunk_size = 10  # <2GB: 10ç§’å—

            print(f"ğŸ” CPU Memory: {total_cpu_gb:.1f}GB total, {available_cpu_gb:.1f}GB available")
            print(f"ğŸ’¾ CPU mode chunk size: {chunk_size}s")
            return chunk_size

        try:
            # è·å–GPUæ˜¾å­˜ä¿¡æ¯
            total_gpu_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB

            # é’ˆå¯¹16GBæ˜¾å¡çš„ä¼˜åŒ–é…ç½®
            if total_gpu_gb >= 15:  # 16GBæ˜¾å¡
                # ä½¿ç”¨ä¿å®ˆçš„æ˜¾å­˜é™åˆ¶ï¼Œç¡®ä¿ç¨³å®šæ€§
                safe_gpu_gb = min(total_gpu_gb * 0.7, self.max_memory_gb)  # ä½¿ç”¨70%æ˜¾å­˜
                print(f"ğŸ¯ 16GB GPU detected, using conservative 70% limit: {safe_gpu_gb:.1f}GB")
            else:
                # å…¶ä»–æ˜¾å¡ä½¿ç”¨80%
                safe_gpu_gb = min(total_gpu_gb * 0.8, self.max_memory_gb)

            # åŒæ—¶è€ƒè™‘GPUæ˜¾å­˜å’ŒCPUå†…å­˜é™åˆ¶
            # éŸ³é¢‘å¤„ç†æ—¶ï¼ŒGPUå¤„ç†ä½†CPUéœ€è¦å­˜å‚¨ä¸­é—´ç»“æœ
            effective_memory = min(safe_gpu_gb, available_cpu_gb * 0.4)  # CPUå†…å­˜çš„40%ç”¨äºç¼“å­˜

            # é’ˆå¯¹16GBæ˜¾å¡çš„å—å¤§å°ä¼˜åŒ– - ç§¯ææ¸…ç†åå¯ä»¥ä½¿ç”¨æ›´å¤§å—
            if total_gpu_gb >= 15:  # 16GBæ˜¾å¡
                if effective_memory >= 10:
                    chunk_size = 120  # 10GB+: 2åˆ†é’Ÿå—ï¼Œé«˜è´¨é‡å¤„ç†
                elif effective_memory >= 8:
                    chunk_size = 90   # 8-10GB: 1.5åˆ†é’Ÿå—
                elif effective_memory >= 6:
                    chunk_size = 75   # 6-8GB: 1.25åˆ†é’Ÿå—
                elif effective_memory >= 4:
                    chunk_size = 60   # 4-6GB: 1åˆ†é’Ÿå—
                else:
                    chunk_size = 45   # <4GB: 45ç§’å—
            else:
                # å…¶ä»–æ˜¾å¡çš„é…ç½®
                if effective_memory >= 8:
                    chunk_size = 45  # 8GB+: 45ç§’å—
                elif effective_memory >= 6:
                    chunk_size = 30  # 6-8GB: 30ç§’å—
                elif effective_memory >= 4:
                    chunk_size = 20  # 4-6GB: 20ç§’å—
                elif effective_memory >= 2:
                    chunk_size = 15  # 2-4GB: 15ç§’å—
                else:
                    chunk_size = 10  # <2GB: 10ç§’å—

            print(f"ğŸ” GPU Memory: {total_gpu_gb:.1f}GB total, using {safe_gpu_gb:.1f}GB")
            print(f"ğŸ” CPU Memory: {total_cpu_gb:.1f}GB total, {available_cpu_gb:.1f}GB available")
            print(f"ğŸ’¾ Effective memory limit: {effective_memory:.1f}GB, chunk size: {chunk_size}s")
            return chunk_size

        except Exception as e:
            print(f"âš ï¸ Could not detect GPU memory: {e}, using conservative chunk size")
            # å›é€€åˆ°ä»…åŸºäºCPUå†…å­˜çš„è®¡ç®—
            if available_cpu_gb >= 4:
                return 20
            elif available_cpu_gb >= 2:
                return 15
            else:
                return 10

    def _aggressive_gpu_cleanup(self):
        """ç§¯æçš„GPUæ˜¾å­˜æ¸…ç† - é’ˆå¯¹16GBæ˜¾å¡ä¼˜åŒ–"""
        if not torch.cuda.is_available():
            return

        try:
            # æ¸…ç©ºCUDAç¼“å­˜
            torch.cuda.empty_cache()

            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()

            # å¦‚æœå¯ç”¨ç§¯ææ¸…ç†ï¼Œè¿›è¡Œæ›´æ·±åº¦çš„æ¸…ç†
            if self.aggressive_cleanup:
                # åŒæ­¥CUDAæ“ä½œ
                torch.cuda.synchronize()

                # å†æ¬¡æ¸…ç©ºç¼“å­˜
                torch.cuda.empty_cache()

                # è·å–å½“å‰æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
                if hasattr(torch.cuda, 'memory_allocated'):
                    allocated = torch.cuda.memory_allocated() / (1024**3)  # GB
                    cached = torch.cuda.memory_reserved() / (1024**3)  # GB
                    print(f"ğŸ§¹ GPU Memory after cleanup: {allocated:.2f}GB allocated, {cached:.2f}GB cached")

        except Exception as e:
            print(f"âš ï¸ GPU cleanup warning: {e}")

    def _load_model(self):
        """åŠ è½½ Demucs æ¨¡å‹"""
        if self.model is None:
            print(f"ğŸ“¥ Loading Demucs model: {self.model_name}...")
            try:
                self.model = get_model(self.model_name)
                self.model.to(self.device)
                self.model.eval()

                # æ™ºèƒ½ç²¾åº¦ä¼˜åŒ– - 16GBæ˜¾å¡å¯é€‰æ‹©ä¿æŒå•ç²¾åº¦è·å¾—æ›´é«˜è´¨é‡
                self.use_half_precision = False
                if self.device == "cuda":
                    if self.high_quality_mode:
                        # 16GBæ˜¾å¡é«˜è´¨é‡æ¨¡å¼ï¼šä¼˜å…ˆä½¿ç”¨å•ç²¾åº¦
                        print("ğŸ¯ 16GBæ˜¾å¡é«˜è´¨é‡æ¨¡å¼ï¼šä½¿ç”¨å•ç²¾åº¦ï¼Œç¡®ä¿æœ€ä½³éŸ³è´¨")
                        self.use_half_precision = False
                    else:
                        # å…¶ä»–æƒ…å†µå°è¯•åŠç²¾åº¦ä¼˜åŒ–
                        try:
                            # æµ‹è¯•æ¨¡å‹æ˜¯å¦æ”¯æŒåŠç²¾åº¦
                            test_input = torch.randn(1, 2, 1000).half().to(self.device)
                            with torch.no_grad():
                                _ = self.model(test_input)

                            # å¦‚æœæµ‹è¯•æˆåŠŸï¼Œå¯ç”¨åŠç²¾åº¦
                            self.model = self.model.half()
                            self.use_half_precision = True
                            print("âœ… åŠç²¾åº¦ä¼˜åŒ–å·²å¯ç”¨ï¼Œæ˜¾å­˜ä½¿ç”¨å‡å°‘50%")

                        except Exception as e:
                            print(f"âš ï¸ åŠç²¾åº¦ä¸å…¼å®¹ï¼Œä½¿ç”¨å•ç²¾åº¦: {e}")
                            self.use_half_precision = False

                print("âœ… Model loaded successfully")
            except Exception as e:
                raise RuntimeError(f"Failed to load Demucs model: {e}")

    def separate_audio_file(self, audio_path: str):
        """ä½¿ç”¨ Python API åˆ†ç¦»éŸ³é¢‘æ–‡ä»¶ - ä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨"""
        self._load_model()

        print(f"ğŸµ Using Demucs Python API to separate audio...")

        try:
            # åŠ è½½éŸ³é¢‘ä¿¡æ¯
            info = torchaudio.info(audio_path)
            sample_rate = info.sample_rate
            total_frames = info.num_frames
            duration = total_frames / sample_rate

            print(f"ğŸ“Š Audio info: {duration:.1f}s, {sample_rate}Hz, {info.num_channels} channels")

            # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†å—å¤„ç†
            if duration <= self.chunk_size:
                return self._process_single_chunk(audio_path)
            else:
                return self._process_with_chunks(audio_path, sample_rate, total_frames)

        except Exception as e:
            raise RuntimeError(f"Demucs separation failed: {e}")
        finally:
            # ç§¯ææ¸…ç†GPUæ˜¾å­˜
            self._aggressive_gpu_cleanup()

    def _process_single_chunk(self, audio_path: str):
        """å¤„ç†å•ä¸ªéŸ³é¢‘å—"""
        print("ğŸ”„ Processing as single chunk...")

        try:
            # åŠ è½½éŸ³é¢‘
            waveform, sample_rate = torchaudio.load(audio_path)

            # ç¡®ä¿éŸ³é¢‘æ˜¯ç«‹ä½“å£°
            if waveform.shape[0] == 1:
                waveform = waveform.repeat(2, 1)
            elif waveform.shape[0] > 2:
                waveform = waveform[:2]

            # æ™ºèƒ½ç²¾åº¦å¤„ç†
            waveform = waveform.to(self.device)
            if self.use_half_precision:
                waveform = waveform.half()

            # åº”ç”¨æ¨¡å‹è¿›è¡Œåˆ†ç¦»
            with torch.no_grad():
                sources = apply_model(self.model, waveform.unsqueeze(0),
                                      device=self.device, progress=True)[0]

            # æ¸…ç†è¾“å…¥æ•°æ®
            del waveform

            result = self._save_sources(sources, sample_rate, Path(audio_path).stem)

            # æ¸…ç†åˆ†ç¦»ç»“æœ
            del sources

            return result

        finally:
            # ç§¯ææ¸…ç†GPUæ˜¾å­˜
            self._aggressive_gpu_cleanup()

    def _process_with_chunks(self, audio_path: str, sample_rate: int, total_frames: int):
        """åˆ†å—å¤„ç†å¤§éŸ³é¢‘æ–‡ä»¶ - ä½¿ç”¨æµå¼åˆå¹¶é¿å…å†…å­˜æº¢å‡º"""
        chunk_frames = int(self.chunk_size * sample_rate)

        # 16GBæ˜¾å¡é«˜è´¨é‡æ¨¡å¼ä½¿ç”¨æ›´å¤§çš„é‡å åŒºåŸŸ
        if self.high_quality_mode:
            overlap_frames = int(1.0 * sample_rate)  # 1ç§’é‡å ï¼Œæé«˜è´¨é‡
            print("ğŸ¯ 16GBæ˜¾å¡é«˜è´¨é‡æ¨¡å¼ï¼šä½¿ç”¨1ç§’é‡å åŒºåŸŸ")
        else:
            overlap_frames = int(0.5 * sample_rate)  # 0.5ç§’é‡å 

        num_chunks = (total_frames + chunk_frames - 1) // chunk_frames
        print(f"ğŸ§© Processing in {num_chunks} chunks of {self.chunk_size}s each...")

        # åˆå§‹åŒ–ä¸´æ—¶æ–‡ä»¶ç”¨äºæµå¼åˆå¹¶
        source_names = self.model.sources
        temp_files = {name: [] for name in source_names}

        # æ£€æŸ¥å¯ç”¨å†…å­˜ï¼Œå†³å®šåˆå¹¶ç­–ç•¥
        _, available_cpu_gb = self._get_cpu_memory_info()
        use_streaming_merge = available_cpu_gb < 6.0 or num_chunks > 10  # å†…å­˜ä¸è¶³æˆ–å—æ•°å¤ªå¤šæ—¶ä½¿ç”¨æµå¼åˆå¹¶

        if use_streaming_merge:
            print(f"ğŸ’¾ Using streaming merge (Available RAM: {available_cpu_gb:.1f}GB)")
        else:
            print(f"ğŸ’¾ Using in-memory merge (Available RAM: {available_cpu_gb:.1f}GB)")
            accumulated_sources = {name: [] for name in source_names}

        for chunk_idx in range(num_chunks):
            start_frame = chunk_idx * chunk_frames
            end_frame = min(start_frame + chunk_frames + overlap_frames, total_frames)

            print(f"ğŸ“¦ Processing chunk {chunk_idx + 1}/{num_chunks} "
                  f"({start_frame/sample_rate:.1f}s - {end_frame/sample_rate:.1f}s)")

            try:
                # åŠ è½½éŸ³é¢‘å—
                waveform, _ = torchaudio.load(
                    audio_path,
                    frame_offset=start_frame,
                    num_frames=end_frame - start_frame
                )

                # ç¡®ä¿ç«‹ä½“å£°
                if waveform.shape[0] == 1:
                    waveform = waveform.repeat(2, 1)
                elif waveform.shape[0] > 2:
                    waveform = waveform[:2]

                # æ™ºèƒ½ç²¾åº¦å¤„ç†
                waveform = waveform.to(self.device)
                if self.use_half_precision:
                    waveform = waveform.half()

                # åˆ†ç¦»éŸ³é¢‘
                with torch.no_grad():
                    sources = apply_model(self.model, waveform.unsqueeze(0),
                                          device=self.device, progress=False)[0]

                # å¤„ç†é‡å éƒ¨åˆ† - é«˜è´¨é‡æ¨¡å¼ä½¿ç”¨æ›´é•¿çš„æ·¡å…¥æ·¡å‡º
                if self.high_quality_mode:
                    fade_frames = int(0.5 * sample_rate)  # 0.5ç§’æ·¡å…¥æ·¡å‡ºï¼Œæ›´å¹³æ»‘
                else:
                    fade_frames = int(0.25 * sample_rate)  # 0.25ç§’æ·¡å…¥æ·¡å‡º

                if chunk_idx > 0:
                    # ç§»é™¤å‰é‡å 
                    sources = sources[:, :, fade_frames:]

                if chunk_idx < num_chunks - 1:
                    # ç§»é™¤åé‡å 
                    sources = sources[:, :, :-fade_frames]

                # æ ¹æ®ç­–ç•¥å¤„ç†åˆ†ç¦»ç»“æœ
                if use_streaming_merge:
                    # æµå¼åˆå¹¶ï¼šç›´æ¥ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
                    for i, source_name in enumerate(source_names):
                        source_chunk = sources[i].cpu().float()

                        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
                        temp_file = Path(tempfile.gettempdir()) / f"demucs_chunk_{source_name}_{chunk_idx}.wav"
                        torchaudio.save(str(temp_file), source_chunk, sample_rate)
                        temp_files[source_name].append(str(temp_file))

                        # ç«‹å³æ¸…ç†å†…å­˜
                        del source_chunk
                else:
                    # å†…å­˜åˆå¹¶ï¼šç´¯ç§¯åˆ°å†…å­˜ä¸­
                    for i, source_name in enumerate(source_names):
                        source_chunk = sources[i].cpu().float()
                        accumulated_sources[source_name].append(source_chunk)

                # æ¸…ç†å½“å‰å—çš„GPUå†…å­˜
                del sources, waveform
                self._aggressive_gpu_cleanup()  # æ¯ä¸ªå—å¤„ç†åç§¯ææ¸…ç†æ˜¾å­˜

            except Exception as e:
                print(f"âš ï¸ Error processing chunk {chunk_idx + 1}: {e}")
                continue

        # åˆå¹¶æ‰€æœ‰å—
        print("ğŸ”— Merging chunks...")

        if use_streaming_merge:
            return self._merge_from_files(temp_files, sample_rate, Path(audio_path).stem)
        else:
            return self._merge_from_memory(accumulated_sources, sample_rate, Path(audio_path).stem)

    def _merge_from_memory(self, accumulated_sources, sample_rate: int, audio_name: str):
        """ä»å†…å­˜ä¸­åˆå¹¶éŸ³é¢‘å—"""
        source_names = self.model.sources
        final_sources = {}

        for source_name in source_names:
            if accumulated_sources[source_name]:
                try:
                    merged = torch.cat(accumulated_sources[source_name], dim=1)
                    final_sources[source_name] = merged
                    print(f"âœ… Successfully merged {source_name}: {len(accumulated_sources[source_name])} chunks")

                    # æ¸…ç†å†…å­˜
                    del accumulated_sources[source_name]
                    gc.collect()

                except Exception as e:
                    print(f"âš ï¸ Failed to merge {source_name}: {e}")
                    continue
            else:
                print(f"âš ï¸ No chunks found for {source_name}")

        if not final_sources:
            raise RuntimeError("No audio sources were successfully processed")

        return self._save_sources_dict(final_sources, sample_rate, audio_name)

    def _merge_from_files(self, temp_files, sample_rate: int, audio_name: str):
        """ä»ä¸´æ—¶æ–‡ä»¶åˆå¹¶éŸ³é¢‘å— - é¿å…å¤§å†…å­˜å ç”¨"""
        source_names = self.model.sources
        outputs = {}

        for source_name in source_names:
            if temp_files[source_name]:
                try:
                    print(f"ğŸ”— Merging {source_name} from {len(temp_files[source_name])} files...")

                    # åˆ›å»ºæœ€ç»ˆè¾“å‡ºæ–‡ä»¶
                    final_file = Path(tempfile.gettempdir()) / f"demucs_{source_name}_{audio_name}.wav"

                    # ä½¿ç”¨FFmpegè¿›è¡Œæ–‡ä»¶çº§åˆå¹¶ï¼Œé¿å…å¤§å†…å­˜å ç”¨
                    if len(temp_files[source_name]) == 1:
                        # åªæœ‰ä¸€ä¸ªæ–‡ä»¶ï¼Œç›´æ¥é‡å‘½å
                        shutil.move(temp_files[source_name][0], str(final_file))
                    else:
                        # å¤šä¸ªæ–‡ä»¶ï¼Œä½¿ç”¨FFmpegåˆå¹¶
                        self._concat_audio_files(temp_files[source_name], str(final_file))

                    outputs[source_name] = str(final_file)
                    print(f"âœ… Successfully merged {source_name}")

                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    for temp_file in temp_files[source_name]:
                        try:
                            if os.path.exists(temp_file) and temp_file != str(final_file):
                                os.remove(temp_file)
                        except Exception as e:
                            print(f"âš ï¸ Could not remove temp file {temp_file}: {e}")

                except Exception as e:
                    print(f"âš ï¸ Failed to merge {source_name}: {e}")
                    # æ¸…ç†å¤±è´¥çš„ä¸´æ—¶æ–‡ä»¶
                    for temp_file in temp_files[source_name]:
                        try:
                            if os.path.exists(temp_file):
                                os.remove(temp_file)
                        except:
                            pass
                    continue
            else:
                print(f"âš ï¸ No files found for {source_name}")

        if not outputs:
            raise RuntimeError("No audio sources were successfully processed")

        return outputs

    def _concat_audio_files(self, file_list, output_file):
        """ä½¿ç”¨FFmpegè¿æ¥éŸ³é¢‘æ–‡ä»¶"""
        try:
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶åˆ—è¡¨
            list_file = Path(tempfile.gettempdir()) / f"concat_list_{os.getpid()}.txt"

            with open(list_file, 'w', encoding='utf-8') as f:
                for file_path in file_list:
                    # ä½¿ç”¨ç»å¯¹è·¯å¾„å¹¶è½¬ä¹‰ç‰¹æ®Šå­—ç¬¦
                    abs_path = os.path.abspath(file_path).replace('\\', '/')
                    f.write(f"file '{abs_path}'\n")

            # ä½¿ç”¨FFmpegè¿æ¥æ–‡ä»¶
            (
                ffmpeg
                .input(str(list_file), format='concat', safe=0)
                .output(output_file, acodec='pcm_s16le')
                .overwrite_output()
                .run(quiet=True, capture_stdout=True)
            )

            # æ¸…ç†ä¸´æ—¶åˆ—è¡¨æ–‡ä»¶
            if os.path.exists(list_file):
                os.remove(list_file)

        except Exception as e:
            # å¦‚æœFFmpegå¤±è´¥ï¼Œå›é€€åˆ°PyTorchæ–¹æ³•
            print(f"âš ï¸ FFmpeg concat failed, using PyTorch fallback: {e}")
            self._concat_audio_files_pytorch(file_list, output_file)

    def _concat_audio_files_pytorch(self, file_list, output_file):
        """ä½¿ç”¨PyTorchè¿æ¥éŸ³é¢‘æ–‡ä»¶ï¼ˆå›é€€æ–¹æ³•ï¼‰"""
        try:
            # é€ä¸ªåŠ è½½å¹¶å†™å…¥ï¼Œé¿å…å¤§å†…å­˜å ç”¨
            first_file = True

            for i, file_path in enumerate(file_list):
                waveform, sample_rate = torchaudio.load(file_path)

                if first_file:
                    # ç¬¬ä¸€ä¸ªæ–‡ä»¶ï¼Œåˆ›å»ºæ–°æ–‡ä»¶
                    torchaudio.save(output_file, waveform, sample_rate)
                    first_file = False
                else:
                    # åç»­æ–‡ä»¶ï¼Œè¿½åŠ åˆ°ç°æœ‰æ–‡ä»¶
                    # æ³¨æ„ï¼štorchaudioä¸æ”¯æŒç›´æ¥è¿½åŠ ï¼Œéœ€è¦å…ˆè¯»å–ç°æœ‰æ–‡ä»¶
                    existing_waveform, _ = torchaudio.load(output_file)
                    combined = torch.cat([existing_waveform, waveform], dim=1)
                    torchaudio.save(output_file, combined, sample_rate)

                    # æ¸…ç†å†…å­˜
                    del existing_waveform, combined

                del waveform
                gc.collect()

        except Exception as e:
            raise RuntimeError(f"Failed to concatenate audio files: {e}")

    def _save_sources(self, sources, sample_rate: int, audio_name: str):
        """ä¿å­˜åˆ†ç¦»çš„éŸ³é¢‘æº"""
        outputs = {}
        source_names = self.model.sources

        for i, source_name in enumerate(source_names):
            # ç§»å› CPU å¹¶è½¬æ¢
            source_audio = sources[i].cpu().float()

            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            temp_file = Path(tempfile.gettempdir()) / f"demucs_{source_name}_{audio_name}.wav"

            # ä¿å­˜éŸ³é¢‘
            torchaudio.save(str(temp_file), source_audio, sample_rate)
            outputs[source_name] = str(temp_file)

        return outputs

    def _save_sources_dict(self, sources_dict, sample_rate: int, audio_name: str):
        """ä¿å­˜åˆ†ç¦»çš„éŸ³é¢‘æºå­—å…¸"""
        outputs = {}

        for source_name, source_audio in sources_dict.items():
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            temp_file = Path(tempfile.gettempdir()) / f"demucs_{source_name}_{audio_name}.wav"

            # ä¿å­˜éŸ³é¢‘
            torchaudio.save(str(temp_file), source_audio, sample_rate)
            outputs[source_name] = str(temp_file)

        return outputs


class AudioSeparator:
    """éŸ³é¢‘åˆ†ç¦»å™¨ä¸»ç±»"""

    QUALITY_SETTINGS = {
        'low': {'bitrate': '64k', 'samplerate': '22050', 'description': 'ä½è´¨é‡ - å¿«é€Ÿå¤„ç†'},
        'medium': {'bitrate': '128k', 'samplerate': '44100', 'description': 'ä¸­ç­‰è´¨é‡ - å¹³è¡¡'},
        'high': {'bitrate': '256k', 'samplerate': '48000', 'description': 'é«˜è´¨é‡ - æœ€ä½³æ•ˆæœ'},
        'ultra_high': {'bitrate': '320k', 'samplerate': '48000', 'description': 'è¶…é«˜è´¨é‡ - 16GBæ˜¾å¡ä¸“ç”¨'},
        'ultra_low': {'bitrate': '32k', 'samplerate': '16000', 'description': 'è¶…ä½è´¨é‡ - è¶…å¤§æ–‡ä»¶ä¸“ç”¨'}
    }

    def __init__(self, output_dir: str = "output", model_name: str = "htdemucs", max_memory_gb: float = 8.0, auto_quality: bool = True):
        self.output_dir = Path(output_dir)
        self.model_name = model_name
        self.max_memory_gb = max_memory_gb
        self.auto_quality = auto_quality
        self.separator = None
        self.temp_files = []

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def _get_video_info(self, video_path: str) -> dict:
        """è·å–è§†é¢‘æ–‡ä»¶ä¿¡æ¯"""
        try:
            probe = ffmpeg.probe(video_path)
            video_info = next(s for s in probe['streams'] if s['codec_type'] == 'video')
            audio_info = next((s for s in probe['streams'] if s['codec_type'] == 'audio'), None)

            duration = float(probe['format']['duration'])
            file_size = int(probe['format']['size']) / (1024 * 1024)  # MB

            return {
                'duration': duration,
                'file_size_mb': file_size,
                'video_codec': video_info.get('codec_name', 'unknown'),
                'audio_codec': audio_info.get('codec_name', 'unknown') if audio_info else 'no_audio',
                'width': int(video_info.get('width', 0)),
                'height': int(video_info.get('height', 0))
            }
        except Exception as e:
            print(f"âš ï¸ Could not get video info: {e}")
            return {'duration': 0, 'file_size_mb': 0}

    def _smart_quality_selection(self, video_path: str, requested_quality: str) -> str:
        """æ™ºèƒ½è´¨é‡é€‰æ‹© - 16GBæ˜¾å¡ä¼˜åŒ–ç‰ˆæœ¬"""
        if not self.auto_quality:
            return requested_quality

        info = self._get_video_info(video_path)
        duration_hours = info['duration'] / 3600
        file_size_mb = info['file_size_mb']

        print(f"ğŸ“Š Video analysis: {duration_hours:.1f}h, {file_size_mb:.1f}MB")

        # æ£€æŸ¥æ˜¯å¦ä¸º16GBæ˜¾å¡
        is_16gb_gpu = False
        try:
            import torch
            if torch.cuda.is_available():
                total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                is_16gb_gpu = total_memory >= 15
        except:
            pass

        # 16GBæ˜¾å¡çš„æ™ºèƒ½è´¨é‡é€‰æ‹©è§„åˆ™
        if is_16gb_gpu:
            if duration_hours > 4.0:  # è¶…è¿‡4å°æ—¶çš„è¶…é•¿è§†é¢‘
                recommended_quality = 'high'
                reason = f"16GBæ˜¾å¡é•¿è§†é¢‘ ({duration_hours:.1f}h) - é«˜è´¨é‡å¤„ç†"
                time_saved = "å¤§å—å¤„ç†ï¼Œè´¨é‡ä¼˜å…ˆ"
            elif duration_hours > 2.0:  # è¶…è¿‡2å°æ—¶çš„é•¿è§†é¢‘
                recommended_quality = 'ultra_high'
                reason = f"16GBæ˜¾å¡ä¸­é•¿è§†é¢‘ ({duration_hours:.1f}h) - è¶…é«˜è´¨é‡"
                time_saved = "å……åˆ†åˆ©ç”¨16GBæ˜¾å­˜"
            elif duration_hours > 0.5:  # è¶…è¿‡30åˆ†é’Ÿçš„è§†é¢‘
                recommended_quality = 'ultra_high'
                reason = f"16GBæ˜¾å¡æ ‡å‡†è§†é¢‘ ({duration_hours:.1f}h) - è¶…é«˜è´¨é‡"
                time_saved = "æœ€ä½³éŸ³è´¨ä½“éªŒ"
            else:  # çŸ­è§†é¢‘
                recommended_quality = 'ultra_high'
                reason = f"16GBæ˜¾å¡çŸ­è§†é¢‘ ({duration_hours:.1f}h) - è¶…é«˜è´¨é‡"
                time_saved = "å¿«é€Ÿé«˜è´¨é‡å¤„ç†"
        else:
            # å…¶ä»–æ˜¾å¡çš„å¹³è¡¡è´¨é‡ä¸é€Ÿåº¦è§„åˆ™
            if duration_hours > 3.0:  # è¶…è¿‡3å°æ—¶çš„é•¿è§†é¢‘
                recommended_quality = 'medium'
                reason = f"é•¿è§†é¢‘ ({duration_hours:.1f}h) - å¹³è¡¡è´¨é‡ä¸å¤„ç†æ—¶é—´"
                time_saved = "èŠ‚çœçº¦60%å¤„ç†æ—¶é—´"
            elif duration_hours > 1.5:  # è¶…è¿‡1.5å°æ—¶çš„ä¸­é•¿è§†é¢‘
                recommended_quality = 'medium'
                reason = f"ä¸­é•¿è§†é¢‘ ({duration_hours:.1f}h) - é€‚ä¸­è´¨é‡ï¼Œåˆç†æ—¶é—´"
                time_saved = "èŠ‚çœçº¦50%å¤„ç†æ—¶é—´"
            elif file_size_mb > 1000:  # è¶…è¿‡1GBçš„å¤§æ–‡ä»¶
                recommended_quality = 'medium'
                reason = f"å¤§æ–‡ä»¶ ({file_size_mb:.0f}MB) - ä¼˜åŒ–å¤„ç†æ•ˆç‡"
                time_saved = "èŠ‚çœçº¦40%å¤„ç†æ—¶é—´"
            else:
                recommended_quality = requested_quality  # çŸ­è§†é¢‘ä¿æŒé«˜è´¨é‡
                reason = f"çŸ­è§†é¢‘ ({duration_hours:.1f}h, {file_size_mb:.0f}MB) - ä¿æŒé«˜è´¨é‡"
                time_saved = ""

        # ç»™å‡ºè°ƒæ•´å»ºè®®
        if recommended_quality != requested_quality:
            print(f"ğŸ¯ æ™ºèƒ½è´¨é‡ä¼˜åŒ–: {requested_quality} â†’ {recommended_quality}")
            print(f"   åŸå› : {reason}")
            print(f"   è¯´æ˜: {self.QUALITY_SETTINGS[recommended_quality]['description']}")
            print(f"   âš¡ æ•ˆæœ: {time_saved}ï¼Œè´¨é‡ä»ç„¶å¾ˆå¥½")
            return recommended_quality
        else:
            print(f"âœ… ä¿æŒé«˜è´¨é‡è®¾ç½®: {requested_quality} ({reason})")
            return requested_quality

    def _estimate_processing_time(self, duration_hours: float, quality: str) -> str:
        """ä¼°ç®—å¤„ç†æ—¶é—´ - å¹³è¡¡è´¨é‡ä¸é€Ÿåº¦çš„ç°å®ä¼°ç®—"""
        # åŸºäºå®é™…æµ‹è¯•çš„å¤„ç†æ—¶é—´ä¼°ç®—ï¼ˆåˆ†é’Ÿï¼‰
        base_time = {
            'ultra_low': duration_hours * 6,   # 6åˆ†é’Ÿ/å°æ—¶
            'low': duration_hours * 10,        # 10åˆ†é’Ÿ/å°æ—¶
            'medium': duration_hours * 15,     # 15åˆ†é’Ÿ/å°æ—¶ (æ¨èçš„å¹³è¡¡é€‰æ‹©)
            'high': duration_hours * 25        # 25åˆ†é’Ÿ/å°æ—¶ (é«˜è´¨é‡ä½†è€—æ—¶)
        }

        estimated_minutes = base_time.get(quality, duration_hours * 15)

        # è€ƒè™‘å„ç§ä¼˜åŒ–çš„åŠ é€Ÿæ•ˆæœ
        speedup_factor = 1.0

        # åŠç²¾åº¦ä¼˜åŒ–
        if hasattr(self, 'use_half_precision') and self.use_half_precision:
            speedup_factor *= 0.8  # åŠç²¾åº¦æé€Ÿ20%

        # GPUæ˜¾å­˜å……è¶³æ—¶çš„é¢å¤–åŠ é€Ÿ
        if hasattr(self, 'max_memory_gb') and self.max_memory_gb >= 10:
            speedup_factor *= 0.9  # å¤§æ˜¾å­˜æé€Ÿ10%

        estimated_minutes *= speedup_factor

        # ç”Ÿæˆå‹å¥½çš„æ—¶é—´æ˜¾ç¤º
        if estimated_minutes < 1:
            return "< 1åˆ†é’Ÿ"
        elif estimated_minutes < 60:
            time_str = f"çº¦ {estimated_minutes:.0f}åˆ†é’Ÿ"
        else:
            hours = estimated_minutes / 60
            if hours < 1.5:
                time_str = f"çº¦ {estimated_minutes:.0f}åˆ†é’Ÿ"
            else:
                time_str = f"çº¦ {hours:.1f}å°æ—¶"

        # æ·»åŠ è´¨é‡è¯´æ˜
        quality_note = {
            'medium': " (æ¨èï¼šè´¨é‡å¥½ï¼Œé€Ÿåº¦å¿«)",
            'high': " (é«˜è´¨é‡ï¼Œè¾ƒæ…¢)",
            'low': " (å¿«é€Ÿå¤„ç†)",
            'ultra_low': " (æœ€å¿«é€Ÿåº¦)"
        }.get(quality, "")

        return time_str + quality_note

    def extract_audio_from_video(self, video_path: str, audio_path: str = None,
                                 audio_quality: str = "high") -> str:
        """ä»è§†é¢‘ä¸­æå–éŸ³é¢‘ - æ”¯æŒæ™ºèƒ½è´¨é‡è°ƒæ•´"""
        if not os.path.exists(video_path):
            raise FileNotFoundError(f"Video file not found: {video_path}")

        # æ™ºèƒ½è´¨é‡é€‰æ‹©
        final_quality = self._smart_quality_selection(video_path, audio_quality)

        if audio_path is None:
            video_name = Path(video_path).stem
            audio_path = self.output_dir / f"{video_name}_raw.wav"

        settings = self.QUALITY_SETTINGS[final_quality]

        # è·å–è§†é¢‘ä¿¡æ¯ç”¨äºæ—¶é—´ä¼°ç®—
        info = self._get_video_info(video_path)
        duration_hours = info['duration'] / 3600
        estimated_time = self._estimate_processing_time(duration_hours, final_quality)

        print(f"ğŸ¬ æå–éŸ³é¢‘: {final_quality} è´¨é‡ ({settings['description']})")
        print(f"â±ï¸ é¢„è®¡å¤„ç†æ—¶é—´: {estimated_time}")

        if final_quality != audio_quality:
            print(f"ğŸ”„ è´¨é‡å·²è‡ªåŠ¨è°ƒæ•´: {audio_quality} â†’ {final_quality} (ä¼˜åŒ–å¤§æ–‡ä»¶å¤„ç†)")

        try:
            stream = ffmpeg.input(str(video_path))
            stream = ffmpeg.output(
                stream,
                str(audio_path),
                vn=None,
                acodec='libmp3lame',
                audio_bitrate=settings['bitrate'],
                ar=settings['samplerate'],
                ac=2
            )
            ffmpeg.run(stream, overwrite_output=True, quiet=True)
            print(f"âœ… éŸ³é¢‘æå–å®Œæˆ: {audio_path}")
            print(f"ğŸ“Š è¾“å‡ºå‚æ•°: {settings['bitrate']} @ {settings['samplerate']}Hz")
            return str(audio_path)

        except Exception as e:
            raise RuntimeError(f"FFmpeg failed: {e}")

    def extract_video_from_video(self, video_path: str, video_path_output: str = None) -> str:
        """ä»è§†é¢‘ä¸­æå–æ— å£°è§†é¢‘ï¼ˆç§»é™¤éŸ³é¢‘è½¨é“ï¼‰"""
        if not os.path.exists(video_path):
            raise FileNotFoundError(f"Video file not found: {video_path}")

        if video_path_output is None:
            video_name = Path(video_path).stem
            video_ext = Path(video_path).suffix  # è·å–åŸè§†é¢‘çš„æ‰©å±•å
            video_path_output = self.output_dir / f"{video_name}_silent{video_ext}"

        print(f"ğŸ¬ Extracting silent video...")

        try:
            stream = ffmpeg.input(str(video_path))
            stream = ffmpeg.output(
                stream,
                str(video_path_output),
                an=None,  # ç§»é™¤éŸ³é¢‘
                vcodec='copy'  # å¤åˆ¶è§†é¢‘æµï¼Œä¸é‡æ–°ç¼–ç 
            )
            ffmpeg.run(stream, overwrite_output=True, quiet=True)
            print(f"âœ… Silent video extracted: {video_path_output}")
            return str(video_path_output)

        except Exception as e:
            raise RuntimeError(f"FFmpeg failed to extract silent video: {e}")

    def _load_demucs_model(self) -> None:
        """åŠ è½½ Demucs æ¨¡å‹"""
        if self.separator is None:
            try:
                self.separator = DemucsWrapper(self.model_name, self.max_memory_gb)
            except Exception as e:
                raise RuntimeError(f"Failed to load model: {e}")

    def separate_audio(self, audio_path: str, output_vocal: str = None,
                       output_background: str = None) -> Tuple[str, str]:
        """åˆ†ç¦»éŸ³é¢‘æº"""
        if not os.path.exists(audio_path):
            raise FileNotFoundError(f"Audio file not found: {audio_path}")

        self._load_demucs_model()
        output_vocal, output_background = self._prepare_output_paths(
            audio_path, output_vocal, output_background
        )

        print("ğŸµ Separating audio sources...")

        try:
            outputs = self.separator.separate_audio_file(audio_path)
            self._process_separation_outputs(outputs, output_vocal, output_background)
            print("âœ¨ Audio separation completed!")
            return str(output_vocal), str(output_background)

        except Exception as e:
            raise RuntimeError(f"Audio separation failed: {e}")
        finally:
            gc.collect()

    def _prepare_output_paths(self, audio_path: str, output_vocal: str = None,
                              output_background: str = None) -> Tuple[Path, Path]:
        """å‡†å¤‡è¾“å‡ºè·¯å¾„"""
        audio_name = Path(audio_path).stem

        if output_vocal is None:
            output_vocal = self.output_dir / f"{audio_name}_vocal.wav"
        else:
            output_vocal = Path(output_vocal)

        if output_background is None:
            output_background = self.output_dir / f"{audio_name}_background.wav"
        else:
            output_background = Path(output_background)

        return output_vocal, output_background

    def _process_separation_outputs(self, outputs: dict, output_vocal: Path,
                                    output_background: Path) -> None:
        """å¤„ç†éŸ³é¢‘åˆ†ç¦»çš„è¾“å‡ºæ–‡ä»¶"""
        try:
            if 'vocals' not in outputs:
                raise RuntimeError("Vocals track not found")

            shutil.copy2(outputs['vocals'], str(output_vocal))
            print(f"âœ… Vocals: {output_vocal}")

            background_sources = [path for name, path in outputs.items() if name != 'vocals']
            if background_sources:
                shutil.copy2(background_sources[0], str(output_background))
                print(f"âœ… Background: {output_background}")
            else:
                print("âš ï¸ No background tracks found")

        finally:
            self._cleanup_temp_files(outputs)

    def _cleanup_temp_files(self, outputs: dict = None) -> None:
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        files_to_clean = []

        if outputs:
            files_to_clean.extend(outputs.values())

        files_to_clean.extend(self.temp_files)

        for temp_file in files_to_clean:
            try:
                if os.path.exists(temp_file):
                    os.remove(temp_file)
                    print(f"ğŸ—‘ï¸ Cleaned temp file: {os.path.basename(temp_file)}")
            except Exception as e:
                print(f"âš ï¸ Failed to clean temp file {temp_file}: {e}")

        self.temp_files.clear()

    def process_video(self, video_path: str, audio_quality: str = "medium") -> Dict[str, str]:
        """å®Œæ•´çš„è§†é¢‘éŸ³é¢‘åˆ†ç¦»æµç¨‹"""
        print(f"ğŸš€ Processing: {Path(video_path).name}")

        try:
            raw_video = self.extract_video_from_video(video_path)
            raw_audio = self.extract_audio_from_video(video_path, audio_quality=audio_quality)
            normalized_audio = self.normalize_audio_volume(raw_audio)
            vocal_audio, background_audio = self.separate_audio(normalized_audio)

            result = {
                'raw_video': raw_video,
                'raw_audio': raw_audio,
                'vocal_audio': vocal_audio,
                'background_audio': background_audio
            }

            print("ğŸ‰ Processing completed!")
            return result

        except Exception as e:
            print(f"âŒ Processing failed: {e}")
            raise

    @staticmethod
    def normalize_audio_volume(audio_path: str, target_db: float = -20.0) -> str:
        """
        æ ‡å‡†åŒ–éŸ³é¢‘éŸ³é‡

        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            target_db: ç›®æ ‡éŸ³é‡ (dB)

        Returns:
            str: æ ‡å‡†åŒ–åçš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„

        Raises:
            ImportError: pydub ä¸å¯ç”¨
            FileNotFoundError: éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨
        """
        if AudioSegment is None:
            raise ImportError("pydub is required for audio normalization. "
                              "Install with: pip install pydub")

        if not os.path.exists(audio_path):
            raise FileNotFoundError(f"Audio file not found: {audio_path}")

        print(f"ğŸ”Š Normalizing audio volume to {target_db}dB...")

        try:
            audio = AudioSegment.from_file(audio_path)
            original_db = audio.dBFS
            change_in_dBFS = target_db - original_db
            normalized_audio = audio.apply_gain(change_in_dBFS)

            # è¦†ç›–åŸæ–‡ä»¶
            normalized_audio.export(audio_path, format="wav")
            print(f"âœ… Audio normalized from {original_db:.1f}dB to {target_db:.1f}dB")

            return audio_path
        except Exception as ex:
            raise RuntimeError(f"Failed to normalize audio: {ex}")

    def cleanup(self) -> None:
        """æ¸…ç†æ¨¡å‹å’Œé‡Šæ”¾å†…å­˜"""
        try:
            self._cleanup_temp_files()

            if self.separator is not None:
                del self.separator
                self.separator = None
            gc.collect()
        except Exception:
            pass

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.cleanup()


def parse_srt_file(srt_path: str) -> List[Dict]:
    """è§£æ SRT å­—å¹•æ–‡ä»¶"""
    if not os.path.exists(srt_path):
        raise FileNotFoundError(f"SRT file not found: {srt_path}")

    encodings = ['utf-8', 'utf-8-sig', 'gbk', 'gb2312', 'cp1252']
    content = None

    for encoding in encodings:
        try:
            with open(srt_path, 'r', encoding=encoding) as file:
                content = file.read()
            print(f"âœ… SRT file loaded with {encoding} encoding")
            break
        except UnicodeDecodeError:
            continue

    if content is None:
        raise RuntimeError(f"Could not decode SRT file with any supported encoding")

    subtitles = []
    for block in content.strip().split('\n\n'):
        lines = [line.strip() for line in block.split('\n') if line.strip()]
        if len(lines) < 3:
            continue

        try:
            number = int(lines[0])
            start_time, end_time = lines[1].split(' --> ')
            start_time = _parse_timestamp(start_time)
            end_time = _parse_timestamp(end_time)
            duration = (end_time - start_time).total_seconds()
            text = ' '.join(lines[2:])
            text = re.sub(r'[ï¼ˆ\(][^ï¼‰\)]*[ï¼‰\)]', '', text).strip()
            text = text.replace('-', '').strip()

            subtitles.append({
                'number': number,
                'start_time': start_time,
                'end_time': end_time,
                'duration': duration,
                'text': text
            })
        except (ValueError, IndexError):
            continue

    print(f"âœ… Parsed {len(subtitles)} subtitle entries")
    return subtitles


def _parse_timestamp(timestamp_str: str) -> datetime.datetime:
    """è§£æ SRT æ—¶é—´æˆ³"""
    timestamp_str = timestamp_str.replace(',', '.')
    return datetime.datetime.strptime(timestamp_str, '%H:%M:%S.%f')


def _timestamp_to_seconds(timestamp: datetime.datetime) -> float:
    """å°†æ—¶é—´æˆ³è½¬æ¢ä¸ºç§’æ•°"""
    return (timestamp.hour * 3600 + timestamp.minute * 60 +
            timestamp.second + timestamp.microsecond / 1000000)


def separate_video_audio(video_path: str, output_dir: str = "output",
                         audio_quality: str = "high", model_name: str = "htdemucs",
                         max_memory_gb: float = 8.0, auto_quality: bool = True) -> Dict[str, str]:
    """
    ä»è§†é¢‘åˆ†ç¦»éŸ³é¢‘å’Œäººå£°è½¨é“

    Args:
        video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        audio_quality: éŸ³é¢‘è´¨é‡ (ä¼šæ ¹æ®æ–‡ä»¶å¤§å°è‡ªåŠ¨è°ƒæ•´)
        model_name: Demucs æ¨¡å‹åç§°
        max_memory_gb: æœ€å¤§æ˜¾å­˜ä½¿ç”¨é™åˆ¶(GB)
        auto_quality: æ˜¯å¦å¯ç”¨æ™ºèƒ½è´¨é‡è°ƒæ•´

    Returns:
        Dict[str, str]: åŒ…å«æ‰€æœ‰è¾“å‡ºæ–‡ä»¶è·¯å¾„çš„å­—å…¸
    """
    with AudioSeparator(output_dir=output_dir, model_name=model_name,
                       max_memory_gb=max_memory_gb, auto_quality=auto_quality) as separator:
        return separator.process_video(video_path, audio_quality=audio_quality)


def split_audio_by_subtitles(audio_path: str, srt_path: str, segments_output_dir: str = None) -> List[Dict]:
    """
    æ ¹æ®å­—å¹•æ–‡ä»¶åˆ†å‰²éŸ³é¢‘
    
    Args:
        audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        srt_path: SRT å­—å¹•æ–‡ä»¶è·¯å¾„
        segments_output_dir: è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        List[Dict]: åˆ†å‰²åçš„éŸ³é¢‘ç‰‡æ®µä¿¡æ¯
    """
    segments_output_dir = Path("segments") if segments_output_dir is None else Path(segments_output_dir)

    segments_output_dir.mkdir(exist_ok=True)

    print(f"ğŸ“ Parsing subtitle file: {srt_path}")
    subtitles = parse_srt_file(srt_path)

    print(f"ğŸµ Loading audio file: {audio_path}")
    audio = AudioSegment.from_file(audio_path)
    audio_duration_ms = len(audio)
    audio_duration_s = audio_duration_ms / 1000.0

    print(f"ğŸµ Audio duration: {audio_duration_s:.2f} seconds")
    segments = []

    print(f"âœ‚ï¸ Splitting audio into {len(subtitles)} segments...")

    for subtitle in subtitles:
        start_ms = int(_timestamp_to_seconds(subtitle['start_time']) * 1000)
        end_ms = int(_timestamp_to_seconds(subtitle['end_time']) * 1000)

        # ç›´æ¥æŒ‰å­—å¹•æ—¶é—´åˆ†å‰²
        segment = audio[start_ms:end_ms]

        segment_filename = f"segment_{subtitle['number']:06d}.wav"
        segment_path = segments_output_dir / segment_filename

        segment.export(str(segment_path), format="wav")

        segments.append({
            'number': subtitle['number'],
            'text': subtitle['text'],
            'duration': subtitle['duration'],
            'file_path': str(segment_path),
            'start_time': start_ms / 1000.0,
            'end_time': end_ms / 1000.0
        })

    print(f"âœ… Created {len(segments)} segments in {segments_output_dir}")
    return segments